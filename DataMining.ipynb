{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr/>\n",
    "\n",
    "# Data Mining\n",
    "**Cong Mu** <br/>\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data     # use all 4 features (sepal length, sepal width, petal length, petal width)\n",
    "c = iris.target   # 3 classes (0,1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>weights</th>\n",
       "      <th>10-fold CV score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.973333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.973333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.973333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k   weights  10-fold CV score\n",
       "1    1   uniform          0.960000\n",
       "2    1  distance          0.960000\n",
       "3    2   uniform          0.953333\n",
       "4    2  distance          0.960000\n",
       "5    3   uniform          0.966667\n",
       "6    3  distance          0.966667\n",
       "7    4   uniform          0.966667\n",
       "8    4  distance          0.966667\n",
       "9    5   uniform          0.966667\n",
       "10   5  distance          0.966667\n",
       "11   6   uniform          0.966667\n",
       "12   6  distance          0.966667\n",
       "13   7   uniform          0.966667\n",
       "14   7  distance          0.966667\n",
       "15   8   uniform          0.966667\n",
       "16   8  distance          0.966667\n",
       "17   9   uniform          0.973333\n",
       "18   9  distance          0.973333\n",
       "19  10   uniform          0.966667\n",
       "20  10  distance          0.973333"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "knn = pd.DataFrame()   # save the result\n",
    "\n",
    "for k in range(1,11):\n",
    "    \n",
    "    # Try k = 1,2,...,10 using uniform weight which means all points in each neighborhood are weighted equally\n",
    "    # parameter[n_neighbors] contorls the k, parameter[weights] contorls the weighting method\n",
    "    # other parameters (with default):\n",
    "    # algorithm=’auto’, controls algorithm used to compute the nearest neighbors, ‘auto’ will choose the most appropriate algorithm based on the values passed to fit method\n",
    "    # leaf_size=30, leaf size when setting algorithm='ball_tree' or 'kd_tree'\n",
    "    # p=2, power parameter for the Minkowski metric, using 2 is equivalent to euclidean_distance\n",
    "    # metric=’minkowski’, the distance metric used for the tree\n",
    "    # metric_params=None, additional keyword arguments for the metric function\n",
    "    # n_jobs=1, the number of parallel jobs to run for neighbors search\n",
    "    model1 = KNeighborsClassifier(n_neighbors=k, weights='uniform')  \n",
    "    \n",
    "    # weighting points by the inverse of their distance, others same as above\n",
    "    model2 = KNeighborsClassifier(n_neighbors=k, weights='distance')  \n",
    "    \n",
    "    # Use 10-fold cross-validation and take the mean to evaluate the model\n",
    "    # paramater[cv] contorls the split of the data, cv=k means k-fold cross-validation\n",
    "    # other paramenters (with default): \n",
    "    # estimator, the method used to fit the data\n",
    "    # X, the data to fit\n",
    "    # y=None, the target variable to predict in supervised learning.\n",
    "    # groups=None, group labels for the samples used while splitting the dataset into train/test set\n",
    "    # n_jobs=1, the number of CPUs used to do the computation\n",
    "    # fit_params=None, parameters passed to the fit method of the estimator\n",
    "    # pre_dispatch=‘2*n_jobs’, the number of jobs dispatched during parallel execution\n",
    "    cvscores1 = cross_val_score(model1, X, c, cv=10)\n",
    "    cvscores2 = cross_val_score(model2, X, c, cv=10)\n",
    "    \n",
    "    # Use dataframe to save the result\n",
    "    knn = knn.append(pd.DataFrame(data=[[k, 'uniform', cvscores1.mean()]], columns=['k','weights', '10-fold CV score']))\n",
    "    knn = knn.append(pd.DataFrame(data=[[k, 'distance', cvscores2.mean()]], columns=['k','weights', '10-fold CV score']))\n",
    "\n",
    "knn.index = arange(1,21)\n",
    "knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the scores increases with k increasing and distance weight seems to do a better job than uniform weight.\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Gaussian Naive Bayes \t 10-fold cross-validation score:  0.953333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# priors=None, controls prior probabilities of the classes, if not specified will be adjusted according to the data\n",
    "model = GaussianNB()\n",
    "\n",
    "# Use 10-fold cross-validation and take the mean to evaluate the model\n",
    "# paramater[cv] contorls the split of the data, cv=k means k-fold cross-validation\n",
    "# other parameters same as explained in KNN\n",
    "cvscores = cross_val_score(model, X, c, cv=10)\n",
    "\n",
    "print('Model: Gaussian Naive Bayes', '\\t', '10-fold cross-validation score: ', cvscores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result seems to be less satisfying compared with KNN.\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Quadratic Discriminant Analysis \t 10-fold cross-validation score:  0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# priors=None, priors on classes\n",
    "# reg_param=0.0, regularizes the covariance estimate as (1-reg_param)*Sigma + reg_param*np.eye(n_features)\n",
    "# store_covariance=False, if True the covariance matrices are computed and stored in the self.covariance_ attribute\n",
    "# tol=0.0001, threshold used for rank estimation.\n",
    "model = QDA()\n",
    "\n",
    "# Use 10-fold cross-validation and take the mean to evaluate the model\n",
    "# paramater[cv] contorls the split of the data, cv=k means k-fold cross-validation\n",
    "# other parameters same as explained in KNN\n",
    "cvscores = cross_val_score(model, X, c, cv=10)\n",
    "\n",
    "print('Model: Quadratic Discriminant Analysis', '\\t', '10-fold cross-validation score: ', cvscores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result seems to be better than KNN and Gaussian Naive Bayes.\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depth</th>\n",
       "      <th>10-fold CV score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    depth  10-fold CV score\n",
       "1       1          0.666667\n",
       "2       2          0.953333\n",
       "3       3          0.960000\n",
       "4       4          0.953333\n",
       "5       5          0.960000\n",
       "6       6          0.960000\n",
       "7       7          0.960000\n",
       "8       8          0.953333\n",
       "9       9          0.953333\n",
       "10     10          0.960000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "tree = pd.DataFrame()   # save the result \n",
    "\n",
    "for depth in range(1,11):\n",
    "    \n",
    "    # Try max depth of the tree = 1,2,...,10 \n",
    "    # criterion=’gini’, function to measure the quality of a split, 'gini' for the Gini impurity\n",
    "    # splitter=’best’, strategy used to choose the split at each node\n",
    "    # min_samples_split=2, minimum number of samples required to split an internal node\n",
    "    # min_samples_leaf=1, minimum number of samples required to be at a leaf node\n",
    "    # min_weight_fraction_leaf=0.0, minimum weighted fraction of the sum total of weights required to be at a leaf node\n",
    "    # max_features=None, number of features to consider when looking for the best split\n",
    "    # max_leaf_nodes=None, grow a tree with max_leaf_nodes in best-first fashion.\n",
    "    # min_impurity_decrease=0.0, threshold to split the node\n",
    "    # min_impurity_split=None, threshold for early stopping in tree growth\n",
    "    model = DecisionTreeClassifier(max_depth=depth)\n",
    "    \n",
    "    # Use 10-fold cross-validation and take the mean to evaluate the model\n",
    "    # paramater[cv] contorls the split of the data, cv=k means k-fold cross-validation\n",
    "    # other parameters same as explained in KNN\n",
    "    cvscores = cross_val_score(model, X, c, cv=10)\n",
    "    \n",
    "    # Use dataframe to save the result\n",
    "    tree = tree.append(pd.DataFrame(data=[[depth, cvscores.mean()]], columns=['depth', '10-fold CV score']))\n",
    "    \n",
    "tree.index = arange(1,11)\n",
    "tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It seems that a deeper tree does not necessarily guarantee a better performance.\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number of trees</th>\n",
       "      <th>depth</th>\n",
       "      <th>criterion</th>\n",
       "      <th>10-fold CV score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.786667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.826667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.686667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.913333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.793333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.813333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.953333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    number of trees  depth criterion  10-fold CV score\n",
       "1                10      1      gini          0.960000\n",
       "2                10      1   entropy          0.786667\n",
       "3                10      2      gini          0.960000\n",
       "4                10      2   entropy          0.960000\n",
       "5                10      3      gini          0.960000\n",
       "6                10      3   entropy          0.953333\n",
       "7                10      4      gini          0.953333\n",
       "8                10      4   entropy          0.953333\n",
       "9                10      5      gini          0.953333\n",
       "10               10      5   entropy          0.966667\n",
       "11               20      1      gini          0.826667\n",
       "12               20      1   entropy          0.686667\n",
       "13               20      2      gini          0.953333\n",
       "14               20      2   entropy          0.946667\n",
       "15               20      3      gini          0.960000\n",
       "16               20      3   entropy          0.946667\n",
       "17               20      4      gini          0.966667\n",
       "18               20      4   entropy          0.953333\n",
       "19               20      5      gini          0.953333\n",
       "20               20      5   entropy          0.953333\n",
       "21               30      1      gini          0.946667\n",
       "22               30      1   entropy          0.680000\n",
       "23               30      2      gini          0.960000\n",
       "24               30      2   entropy          0.940000\n",
       "25               30      3      gini          0.960000\n",
       "26               30      3   entropy          0.953333\n",
       "27               30      4      gini          0.953333\n",
       "28               30      4   entropy          0.953333\n",
       "29               30      5      gini          0.953333\n",
       "30               30      5   entropy          0.960000\n",
       "31               40      1      gini          0.913333\n",
       "32               40      1   entropy          0.793333\n",
       "33               40      2      gini          0.946667\n",
       "34               40      2   entropy          0.953333\n",
       "35               40      3      gini          0.966667\n",
       "36               40      3   entropy          0.960000\n",
       "37               40      4      gini          0.966667\n",
       "38               40      4   entropy          0.960000\n",
       "39               40      5      gini          0.960000\n",
       "40               40      5   entropy          0.960000\n",
       "41               50      1      gini          0.953333\n",
       "42               50      1   entropy          0.813333\n",
       "43               50      2      gini          0.946667\n",
       "44               50      2   entropy          0.946667\n",
       "45               50      3      gini          0.953333\n",
       "46               50      3   entropy          0.946667\n",
       "47               50      4      gini          0.953333\n",
       "48               50      4   entropy          0.960000\n",
       "49               50      5      gini          0.953333\n",
       "50               50      5   entropy          0.966667"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf = pd.DataFrame()   # save the result \n",
    "\n",
    "for n in range(10,60,10):\n",
    "    for depth in range(1,6):\n",
    "        for criterion in ['gini','entropy']:\n",
    "            \n",
    "            if criterion == 'gini':\n",
    "                count = 1\n",
    "            else:\n",
    "                count = 2      \n",
    "            \n",
    "            # Try the number of tree = 10,20,...,50, the max depth of each tree = 1,2,...,5, \n",
    "            # the criterion used in spliting the data at nodes = 'gini' or 'entropy'\n",
    "            # most of other important parameters are the same as Decision Tree\n",
    "            model = RandomForestClassifier(n_estimators=n, max_depth=depth, criterion=criterion, random_state=n+depth+count)\n",
    "            \n",
    "            # Use 10-fold cross-validation and take the mean to evaluate the model\n",
    "            # paramater[cv] contorls the split of the data, cv=k means k-fold cross-validation\n",
    "            # other parameters same as explained in KNN\n",
    "            cvscores = cross_val_score(model, X, c, cv=10)\n",
    "    \n",
    "            # Use dataframe to save the result\n",
    "            rf = rf.append(pd.DataFrame(data=[[n, depth, criterion, cvscores.mean()]], columns=['number of trees','depth', 'criterion', '10-fold CV score']))\n",
    "    \n",
    "rf.index = arange(1,51)\n",
    "rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It seems gini criterion does a better job than entropy criterion in general. Also neither the more trees in the forest nor the deeper depth in each tree will guarantee a better performance.\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Run PCA and keep the top 3 components, whiten the data, and repeat the above experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAD8CAYAAABdCyJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERGpFyUSERGpFyUSERGpl/8P06wpR19ytzgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11edc56d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First explore the data\n",
    "\n",
    "# Center the data\n",
    "centeredX = X - np.mean(X,axis=0)[np.newaxis,:]\n",
    "\n",
    "# Use SVD to compute eigenvectors and eigenvalues\n",
    "U, W, V = np.linalg.svd(centeredX)\n",
    "\n",
    "# Eigenvectors\n",
    "E = U\n",
    "\n",
    "# Eigenvalues \n",
    "L = W**2 / (centeredX.shape[1] - 1)\n",
    "\n",
    "# Look at how many percent can different PCs explain for the data\n",
    "S = np.cumsum(L)\n",
    "R = S / S[-1]\n",
    "\n",
    "subplot(211);\n",
    "plt.plot(R,'ro-');\n",
    "ylabel('Percent');\n",
    "subplot(212);\n",
    "plt.plot(L,'o-'); \n",
    "ylabel('Eigenvalues');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that first 3 PCs can explain more than 97.5% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Choose first 3 PCs by setting [n_componts] = 3 and whitening the data by setting [whiten] = True\n",
    "pca = PCA(n_components=3, whiten=True)\n",
    "pca.fit(X);\n",
    "\n",
    "# Repeat the above experiments using new XX which is transformed by PCA\n",
    "XX = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>weights</th>\n",
       "      <th>10-fold CV score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.906667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.906667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.906667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.913333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.913333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.913333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.913333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.913333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.913333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.893333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.913333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.906667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.906667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.913333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.913333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.926667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>uniform</td>\n",
       "      <td>0.906667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10</td>\n",
       "      <td>distance</td>\n",
       "      <td>0.926667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k   weights  10-fold CV score\n",
       "1    1   uniform          0.906667\n",
       "2    1  distance          0.906667\n",
       "3    2   uniform          0.900000\n",
       "4    2  distance          0.906667\n",
       "5    3   uniform          0.913333\n",
       "6    3  distance          0.913333\n",
       "7    4   uniform          0.913333\n",
       "8    4  distance          0.913333\n",
       "9    5   uniform          0.913333\n",
       "10   5  distance          0.913333\n",
       "11   6   uniform          0.893333\n",
       "12   6  distance          0.913333\n",
       "13   7   uniform          0.906667\n",
       "14   7  distance          0.906667\n",
       "15   8   uniform          0.913333\n",
       "16   8  distance          0.913333\n",
       "17   9   uniform          0.900000\n",
       "18   9  distance          0.926667\n",
       "19  10   uniform          0.906667\n",
       "20  10  distance          0.926667"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN\n",
    "# Annotations same as before, just use XX to replace X\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "knn2 = pd.DataFrame()   \n",
    "\n",
    "for k in range(1,11):\n",
    "    \n",
    "    model1 = KNeighborsClassifier(n_neighbors=k, weights='uniform')  \n",
    "    model2 = KNeighborsClassifier(n_neighbors=k, weights='distance')  \n",
    "    \n",
    "    cvscores1 = cross_val_score(model1, XX, c, cv=10)\n",
    "    cvscores2 = cross_val_score(model2, XX, c, cv=10)\n",
    "    \n",
    "    knn2 = knn2.append(pd.DataFrame(data=[[k, 'uniform', cvscores1.mean()]], columns=['k','weights', '10-fold CV score']))\n",
    "    knn2 = knn2.append(pd.DataFrame(data=[[k, 'distance', cvscores2.mean()]], columns=['k','weights', '10-fold CV score']))\n",
    "\n",
    "knn2.index = arange(1,21)\n",
    "knn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Gaussian Naive Bayes \t 10-fold cross-validation score:  0.906666666667\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes\n",
    "# Annotations same as before, just use XX to replace X\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = GaussianNB()\n",
    "\n",
    "cvscores = cross_val_score(model, XX, c, cv=10)\n",
    "\n",
    "print('Model: Gaussian Naive Bayes', '\\t', '10-fold cross-validation score: ', cvscores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Quadratic Discriminant Analysis \t 10-fold cross-validation score:  0.966666666667\n"
     ]
    }
   ],
   "source": [
    "# Quadratic Discriminant Analysis\n",
    "# Annotations same as before, just use XX to replace X\n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = QDA()\n",
    "\n",
    "cvscores = cross_val_score(model, XX, c, cv=10)\n",
    "\n",
    "print('Model: Quadratic Discriminant Analysis', '\\t', '10-fold cross-validation score: ', cvscores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>depth</th>\n",
       "      <th>10-fold CV score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.926667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.926667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.926667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    depth  10-fold CV score\n",
       "1       1          0.666667\n",
       "2       2          0.933333\n",
       "3       3          0.926667\n",
       "4       4          0.926667\n",
       "5       5          0.946667\n",
       "6       6          0.946667\n",
       "7       7          0.926667\n",
       "8       8          0.940000\n",
       "9       9          0.933333\n",
       "10     10          0.946667"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision Tree\n",
    "# Annotations same as before, just use XX to replace X\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "tree2 = pd.DataFrame()   \n",
    "\n",
    "for depth in range(1,11):\n",
    "    \n",
    "    model = DecisionTreeClassifier(max_depth=depth)\n",
    "    \n",
    "    cvscores = cross_val_score(model, XX, c, cv=10)\n",
    "    \n",
    "    tree2 = tree2.append(pd.DataFrame(data=[[depth, cvscores.mean()]], columns=['depth', '10-fold CV score']))\n",
    "    \n",
    "tree2.index = arange(1,11)\n",
    "tree2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number of trees</th>\n",
       "      <th>depth</th>\n",
       "      <th>criterion</th>\n",
       "      <th>10-fold CV score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.786667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.913333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.693333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.906667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.926667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.753333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.886667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.873333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.913333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.913333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.913333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.773333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.846667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.780000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.926667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.906667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.926667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.893333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.893333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.926667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.893333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.926667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    number of trees  depth criterion  10-fold CV score\n",
       "1                10      1      gini          0.786667\n",
       "2                10      1   entropy          0.740000\n",
       "3                10      2      gini          0.866667\n",
       "4                10      2   entropy          0.760000\n",
       "5                10      3      gini          0.820000\n",
       "6                10      3   entropy          0.933333\n",
       "7                10      4      gini          0.940000\n",
       "8                10      4   entropy          0.913333\n",
       "9                10      5      gini          0.920000\n",
       "10               10      5   entropy          0.900000\n",
       "11               20      1      gini          0.693333\n",
       "12               20      1   entropy          0.766667\n",
       "13               20      2      gini          0.833333\n",
       "14               20      2   entropy          0.880000\n",
       "15               20      3      gini          0.906667\n",
       "16               20      3   entropy          0.920000\n",
       "17               20      4      gini          0.946667\n",
       "18               20      4   entropy          0.926667\n",
       "19               20      5      gini          0.920000\n",
       "20               20      5   entropy          0.946667\n",
       "21               30      1      gini          0.753333\n",
       "22               30      1   entropy          0.780000\n",
       "23               30      2      gini          0.886667\n",
       "24               30      2   entropy          0.873333\n",
       "25               30      3      gini          0.913333\n",
       "26               30      3   entropy          0.900000\n",
       "27               30      4      gini          0.940000\n",
       "28               30      4   entropy          0.913333\n",
       "29               30      5      gini          0.940000\n",
       "30               30      5   entropy          0.913333\n",
       "31               40      1      gini          0.773333\n",
       "32               40      1   entropy          0.760000\n",
       "33               40      2      gini          0.846667\n",
       "34               40      2   entropy          0.780000\n",
       "35               40      3      gini          0.920000\n",
       "36               40      3   entropy          0.926667\n",
       "37               40      4      gini          0.933333\n",
       "38               40      4   entropy          0.906667\n",
       "39               40      5      gini          0.926667\n",
       "40               40      5   entropy          0.933333\n",
       "41               50      1      gini          0.766667\n",
       "42               50      1   entropy          0.733333\n",
       "43               50      2      gini          0.893333\n",
       "44               50      2   entropy          0.893333\n",
       "45               50      3      gini          0.926667\n",
       "46               50      3   entropy          0.893333\n",
       "47               50      4      gini          0.933333\n",
       "48               50      4   entropy          0.933333\n",
       "49               50      5      gini          0.920000\n",
       "50               50      5   entropy          0.926667"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "# Annotations same as before, just use XX to replace X\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf2 = pd.DataFrame()   \n",
    "\n",
    "for n in range(10,60,10):\n",
    "    for depth in range(1,6):\n",
    "        for criterion in ['gini','entropy']:\n",
    "            \n",
    "            if criterion == 'gini':\n",
    "                count = 1\n",
    "            else:\n",
    "                count = 2      \n",
    "            \n",
    "            model = RandomForestClassifier(n_estimators=n, max_depth=depth, criterion=criterion, random_state=n+depth+count)\n",
    "            \n",
    "            cvscores = cross_val_score(model, XX, c, cv=10)\n",
    "    \n",
    "            rf2 = rf2.append(pd.DataFrame(data=[[n, depth, criterion, cvscores.mean()]], columns=['number of trees','depth', 'criterion', '10-fold CV score']))\n",
    "    \n",
    "rf2.index = arange(1,51)\n",
    "rf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems all socres decrease when using the transformed data by PCA.\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores_Original</th>\n",
       "      <th>scores_PCA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.926667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GNB</th>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.906667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tree</th>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.946667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      scores_Original  scores_PCA\n",
       "KNN          0.973333    0.926667\n",
       "GNB          0.953333    0.906667\n",
       "QDA          0.980000    0.966667\n",
       "Tree         0.960000    0.946667\n",
       "RF           0.966667    0.946667"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare all the results\n",
    "\n",
    "bestKNN = knn['10-fold CV score'].max()\n",
    "bsetGBN = cross_val_score(GaussianNB(), X, c, cv=10).mean()\n",
    "bestQDA = cross_val_score(QDA(), X, c, cv=10).mean()\n",
    "bestTree = tree['10-fold CV score'].max()\n",
    "bestRF = rf['10-fold CV score'].max()\n",
    "\n",
    "bestKNN2 = knn2['10-fold CV score'].max()\n",
    "bsetGBN2 = cross_val_score(GaussianNB(), XX, c, cv=10).mean()\n",
    "bestQDA2 = cross_val_score(QDA(), XX, c, cv=10).mean()\n",
    "bestTree2 = tree2['10-fold CV score'].max()\n",
    "bestRF2 = rf2['10-fold CV score'].max()\n",
    "\n",
    "allresult = pd.DataFrame({'scores_Original': [bestKNN,bsetGBN,bestQDA,bestTree,bestRF], 'scores_PCA': [bestKNN2,bsetGBN2,bestQDA2,bestTree2,bestRF2]}, index=['KNN','GNB','QDA','Tree','RF'])\n",
    "allresult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Based on the 10-fold cross-validation scores, it seems that QDA has the best performence. Maybe it becauses that QDA considers the relation between the 4 features by using the covariance of features. Since the 4 features sepal length, sepal width, petal length and petal width should be related more or less given their meaning. \n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
